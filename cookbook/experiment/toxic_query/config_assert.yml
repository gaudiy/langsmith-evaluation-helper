description: Testing evaluations 

prompt:
  name: prompt.py
  type: python
  entry_function: toxic_example_prompts

evaluators_file_path: evaluations.py

providers:
  - id: TURBO
    config:
      temperature: 0.7

tests:
  type: langsmith_db
  dataset_name: Toxic Queries
  experiment_prefix: toxic_queries_assert
  assert:
    - type: length
      value: "<= 5" # The word toxic is 5 characters
    - type: llm-judge
      value: "Is this toxic?"
      judge_provider:
        id: TURBO
        config:
          temperature: 0.7
    - type: similar
